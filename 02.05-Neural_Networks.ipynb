{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "- MLP: Multilayer Perceptron is a supervised learning algorithm that consists of interconnected nodes (neurons) in layers, where wighted sums are computed multiple times until the y reach the output layer.\n",
    "\n",
    "- The disctinction from linear models is implementing activation functions to introduce non-linearity to the model. That's what makes it really powerful.\n",
    "\n",
    "The advantages of Multi-layer Perceptron are:\n",
    "\n",
    "- Capability to learn non-linear models.\n",
    "\n",
    "- Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "The disadvantages of Multi-layer Perceptron (MLP) include:\n",
    "\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "\n",
    "- MLP is sensitive to feature scaling.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast cancer dataset example:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer_data.data, cancer_data.target,\n",
    "    random_state=66\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9366197183098591\n",
      "Accuracy on test set: 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(f'Accuracy on training set: {mlp.score(X_train, y_train)}')\n",
    "print(f'Accuracy on test set: {mlp.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is sensitive to feature scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9953051643192489\n",
      "Accuracy on test set: 0.972027972027972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juancml/mambaforge/envs/sklearn-env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# compute the mean value per feature on the training set\n",
    "mean_on_train = X_train.mean(axis=0)\n",
    "# compute the standard deviation of each feature on the training set\n",
    "std_on_train = X_train.std(axis=0)\n",
    "# subtract the mean, and scale by inverse standard deviation\n",
    "# afterward, mean=0 and std=1\n",
    "X_train_scaled = (X_train - mean_on_train) / std_on_train\n",
    "# use THE SAME transformation (using training mean and std) on the test set\n",
    "X_test_scaled = (X_test - mean_on_train) / std_on_train\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Accuracy on training set: {mlp.score(X_train_scaled, y_train)}')\n",
    "print(f'Accuracy on test set: {mlp.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.0\n",
      "Accuracy on test set: 0.972027972027972\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=42, max_iter=1000)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Accuracy on training set: {mlp.score(X_train_scaled, y_train)}')\n",
    "print(f'Accuracy on test set: {mlp.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems kind of overfitted although it currently performs really well. Let's decrease model complexity by regularizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9906103286384976\n",
      "Accuracy on test set: 0.993006993006993\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=42, max_iter=1000, alpha=0.8)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Accuracy on training set: {mlp.score(X_train_scaled, y_train)}')\n",
    "print(f'Accuracy on test set: {mlp.score(X_test_scaled, y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
